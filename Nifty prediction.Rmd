---
title: "Nifty trend for next 30Days"
author: "Rajat Kumar"
date: "July 12,2021"
output:
   html_notebook:
    toc: True
    toc_float: True
    number_sections: True
    highlight: kate
    fig_caption: True
    fig_width: 12
    fig_height: 8
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

# Introduction
In this notebook our aim is to find the trend of Nifty for next 30Days using different model's (like knn, prophet and ARIMA) and we also identify the best suited model or a model of best accuracy for our prediction.  

# Definition
**Time Series:** A time series is a sequence of data points that occur in *successive order* over some period of time.An observed time series can be decomposed into three components:  
1. The trend (long term direction)  
2. The seasonal (systematic, calendar related movements)  
3. The irregular (unsystematic, short term fluctuations)  
**Nifty:** It is a market index introduced by the *National Stock Exchange*(Mumbai). It is a blended word – National Stock Exchange and Fifty coined by NSE on 21st April 1996. NIFTY 50 is a benchmark based index and also the flagship of NSE, which showcases the top 50 equity stocks traded in the *stock exchange* out of a total of 1600 stocks.  

Nifty 50 is calculated by taking the weighted value of the 50 stocks listed on NSE and is based on free-float market capitalization. The index value is calculated using market capitalization and reflects the value of the stocks relative to the base period. The market value is calculated as the product of several shares and the market price per share.  
$$
Index\ Value=\frac{Current\ Market\ Value}{(Base\ Market\ Capital) * (Base\ Index\ Value)} 
$$
**Stationary Series:** A stationary process has the property that the mean, variance and autocorrelation structure do not change over time. Stationarity can be defined in precise mathematical terms, but for our purpose we mean a flat looking series, without trend, constant variance over time, a constant autocorrelation structure over time and no periodic fluctuations.  
**Lag:** For some specific time point r, the observation $x_{r-1}$ (i periods back) is called the i-th lag of $x_{r}$. A time series Y generated by back-shifting another time series X by i time steps is also sometime called the i-th lag of X, or an i-lag of X. This transformation is called both the backshifting operator, commonly denoted as B(∙),and the lag operator, commonly denoted as L(∙); thus, L(Xᵣ)=$X_{r-1}$₁. Powers of the operators are defined as $L^{i}(X_{r})=X_{r-1}$  
**AIC:** Akaike’s information criterion (AIC) compares the quality of a set of statistical models to each other. For example, you might be interested in what variables contribute to low socioeconomic status and how the variables contribute to that status. The AIC will take each model and rank them from best to worst. The “best” model will be the one that neither under-fits nor over-fits.Although the AIC will choose the best model from a set, it won’t say anything about absolute quality. In other words, if all of your models are poor, it will choose the best of a bad bunch.  
Akaike’s Information Criterion is usually calculated with software. The basic formula is defined as:
$$
AIC=-2(log-likelihood)+2k
$$
Where:  

* **k** is the number of model parameters (the number of variables in the model plus the intercept)  
* **Log-likelihood** is a measure of model fit. The higher the number, the better the fit. This is usually obtained from statistical output.


**Mean Absolute Percentage Error (MAPE):** The mean absolute percentage error (MAPE) is a measure of how accurate a forecast system is. It measures this accuracy as a percentage, and can be calculated as the average absolute percent error for each time period minus actual values divided by actual values.  
Formula:  
$$
M=\frac{1}{n}\sum_{t=1}^{n}\left|\frac{A_{t}-F{t}}{A_{t}}\right|
$$
Where:  

* **n** is the number of fitted points  
* $A_{t}$ is the actual value,  
* $F_{t}$ is the forecast value,  
* $\sum$ is summation notation (the absolute value is summed for every forecasted point in time).

# Data  
Data for this project is nifty stock data of last 5 years. The data is downloaded from **Yahoo! Finance** which includes variables:  
1. **Date:** Dates for which stock market is open (11-01-2016 to 10-06-2021)  
2. **Open:** Opening price of the nifty on the day  
3. **High:** Maximum price on the day   
4. **Low:** Minimum price on the day  
5. **Close:** Closing price of the nifty on the day  
6. **Adj. close:** Adjusted values incorporate changes resulting from corporate actions such as dividend payments, stock splits, or new share issuance  
7. **Volume:** Volume refers to the number of shares that exchange hands for a stock with a specific period

# Library
```{r} 
library(ggplot2)   #graphics
library(gridExtra) #graphics
library(tsfknn)    #K-NN Model
library(vars)      #lag
library(prophet)   #prophet model
library(forecast)  #Arima model
library(tseries)   #ADF test
```

# Data Importing  
```{r}
library(readr)
Nifty <- read_csv("Nifty_50.csv")
head(Nifty)
```

# Data Overview{.tabset}
## Summary
```{r}
summary(Nifty)
```

## Structure
```{r}
str(Nifty)
```
For our analysis we need only date variable and Close variable. So, we made a data set of date and close variable only.

# Data Cleaning{.tabset}
```{r}
Nifty$Date<-as.Date(Nifty$Date,format="%d-%m-%Y")
Nifty$Close<-as.numeric(Nifty$Close)
Nifty<-Nifty[,c(1,5)]
```

## Summary
```{r}
summary(Nifty)
```
As, we see their are 6 NA value's in data. So, we remove those observation.

```{r}
Nifty<-Nifty[complete.cases(Nifty),]
table(is.na(Nifty))
```
So, we prepare our data set for our analysis.

## Structure
```{r}
str(Nifty)
```

# Time Series
```{r}
Time_series<-ts(Nifty$Close,start = c(2016),frequency = 245.5)
head(Time_series)
```

# Analysis of Time series{.tabset}
## Plot of Time Series
```{r}
autoplot(Time_series,ylab = "Close Price")
```
## Component of Time Series
```{r echo=TRUE}
plot(decompose(Time_series))
```
As, we clearly see a increasing trend of the Nifty in last 5 year and the seasonal decreased is due to the **Covid** because in april 2020 stock market decreased very sharply as we see in observed graph or plot of time series.  
Now we apply model on the time series:


# K-Nearest Neighbors(K-NN)
KNN regression simply holds a collection of training instances. The i-th training instance consists of a vector of n features: $(f_{1}^{i},f_{2}^{i},...,f_{n}^{i})$ , describing the instance and an associated target vector of m attributes: $(t_{1}^{i},t_{2}^{i},...,t_{m}^{i})$ . Given a new instance, whose features are known $(q_{1},q_{2},...,q_{n)}$ but whose target is unknown, the features of the new instance are used to find its k most similar training instances according to the vectors of features and a similarity or distance metric. For example, assuming that the similarity metric is the Euclidean distance, the distance between the new instance and the i-th training instance is computed as follows:
$$
\sqrt{\sum_{x=1}^{n} (f_{x}^{i} - q_{x})^2}
$$
The k training instances that are closest to the new instance are considered their k most similar instances or k nearest neighbors. KNN is based on learning by analogy. Given a new instance, we think that the targets of its nearest neighbors are probably similar to its unknown target. This way, the targets of the nearest neighbors are aggregated to predict the target of the new instance. For example, assuming that the targets or the k nearest neighbors are the vectors: $t^{1},t^{2},...,t^{k}$ , they can be averaged to predict the target of the new instance as:
$$
\sum_{i=1}^{k} \frac{t^{i}}{k}
$$
In short, KNN stores a collection of training instances described by n features. Each training instance represents a point in an n-dimensional space. Given a new instance, KNN finds its k closest instances in the n-dimensional space in the hope that their targets are similar to its unknown target.  

## Lag Value
1st we find the optimum lags in our time series. For this we use **VARselect** command which comes under **vars** package. It gives us **AIC,HQ,SC,FPE** for different lags. **Corresponding to lower AIC value we obtain optimal lag value**. We take **lag.max=220** which is enough for our series.
```{r echo=TRUE}
lag<-VARselect(Time_series,lag.max = 220)
AIC<-data.frame("lag"=1:220,"AIC"=lag[[2]][1,])
value_of_lag<-subset(AIC,AIC==min(AIC))[1,]
paste("Optimum value of lag is ",value_of_lag$lag,"with AIC ",round(value_of_lag$AIC,5))
```

## k-value
Now, we find the k value for fit our model.  
Their is no technique to obtain the value of k. So, i made a sequence of knn model with lag 12 and compare them on the basis of MAPE. Finally we select the k value which model have least MAPE. And, proceed this model further for analysis and prediction.
```{r}
MAPE<-NULL
model.knn<-for(i in 1:60){
  knn.mod<-knn_forecasting(Time_series,h=30,lags = 1:12,k=i,
                           msas = "recursive",transform = "additive")
  MAPE[i]<-rolling_origin(knn.mod,rolling = F)$global_accu[3]
  }
k_table<-data.frame(k=1:60,MAPE)
value_of_k<-subset(k_table,MAPE==min(MAPE))[1,]
paste("Optimum value of k is ",value_of_k$k," with MAPE ",round(value_of_k$MAPE,5))
```
Now we are set to make a knn model

## K-NN Model
```{r}
knn_model<-knn_forecasting(Time_series,h=30,lags = 1:12,k=2,
                           msas = "recursive",transform = "additive")
summary(knn_model)
```

## Analysis of Model
```{r}
knn_ro<-rolling_origin(knn_model,rolling = F)
knn_pred<-as.vector(knn_ro$predictions)
knn_error<-as.vector(knn_ro$errors)
data.frame("actual"=Nifty$Close[1301:1330],"predict"=knn_pred,"Error"=knn_error)
knn_ro$global_accu
knn_g1<-ggplot()+ggtitle("Actual Value v/s Model Value")+
  geom_line(aes(x=Nifty$Date[1301:1330],y=Nifty$Close[1301:1330]))+
  geom_line(aes(x=Nifty$Date[1301:1330],y=knn_pred),col="red")+
  ylab("Close Price")+xlab("Date")
knn_g2<-ggplot()+ggtitle("Residual")+
  geom_line(aes(x=Nifty$Date[1301:1330],y=knn_error),col="brown")+
  ylab("Close Price")+xlab("Date")
grid.arrange(knn_g1,knn_g2,ncol=2)
```
## Model Prediction for 30 Days
```{r}
knn_model$prediction
date_seq30<-seq(as.Date("2021/06/10"),by = "days",length.out=30)
knn_forcast<-as.vector(knn_model$prediction)
ggplot()+ggtitle("Forcast Value")+
  geom_line(aes(x=Nifty$Date[1000:1330],y=Nifty$Close[1000:1330]),col="green")+
  geom_line(aes(x=date_seq30,y=knn_forcast),col="red")+
  xlab("Date")+ylab("Close Price")
```
**Red** and **green** lines are based on forecasting and actual values respectively.  
We clearly see that knn model give us a increasing trend.

# Prophet
Facebook Prophet is a tool developed by Facebook for forecasting time series objects or data. It helps businesses to learn the behavior of their products by forecasting prices, sales, or weather. Facebook Prophet tool is based on decomposable model i.e., trend, seasonality and holidays that helps in making more accurate predictive models with these constraints.  
**Mathematical Equation of Prophet Model**  
$$
y(t)=g(t)+s(t)+h(t)+e(t)
$$
where,  

* **y(t)** refers to the forecast  
* **g(t)** refers to the trend  
* **s(t)** refers to the seasonality  
* **h(t)** refers to the holidays for the forecast  
* **e(t)** refers to the error term while forecasting  

Some Important Terms Used in Facebook Prophet Model  

* **Trend**  
A trend is a shift in development either in increment or decrement direction.   **Mathematically,**
$$
g(t) = \frac{C}{1+e^{-k(t-m)}}
$$
where,  
    + **C** indicates the carry capacity  
    + **k** indicates the growth  
    + **m** indicates the offset parameter  

* **Seasonality**  
Seasonality is a feature of time series object that occurs at a particular time/season and changes the trend.  

* **Holidays**  
Holidays are a time period that changes a lot to the business. It can make a profit or loss depending upon the business.  

## Data preparation
For the prophet model we change name of date and close price to "ds" and "y" respectively.
```{r}
df<-data.frame("ds"=Date,"y"=Close)
head(df)
```
## Propet Model
```{r}
prophet_model<-prophet(df,growth = "linear")
```
This model give as a lot of info which we seen using command **prophet_model** and **summary(prophet_model).** For prediction we make a data frame for next 30 days.
```{r}
future<-make_future_dataframe(prophet_model,periods = 30)
tail(future)
```
Now, are set for prediction for next 30 days

## Forecast for 30 Days
```{r}
prophet_forcast<-predict(prophet_model,future)
tail(prophet_forcast[c("ds","yhat","yhat_lower","yhat_upper","trend_lower","trend_upper")],10)
```
**yhat** gives us the forecast value.  
**yhat_lower** and **yhat_upper** gives us the range of forecasting value.   
**trend_lower** and **trend_upper** gives us the trend.  

## Forcasting Plot{.tabset}
### Prophet Model
```{r}
dyplot.prophet(prophet_model,prophet_forcast)
```

### Model Components
```{r}
prophet_plot_components(prophet_model,prophet_forcast)
```

## Analysis of Model
```{r}
ds_cv<-cross_validation(prophet_model,horizon = 365,units = 'days')
head(ds_cv)
acc<-performance_metrics(ds_cv)
head(acc)
plot_cross_validation_metric(ds_cv,metric = "mape")
```

# Autoregressive Integrated Moving Average (ARIMA)  
ARIMA models are used because they can reduce a non-stationary series to a stationary series using a sequence of *differencing* steps.  
if we apply the difference operator to a random walk series {$x_{t}$} (a non-stationary series) we are left with white noise {$w_{t}$} (a stationary series):
$$
\Delta{x_{t}}=x_{t}-x_{t-1}=w_{t}
$$
ARIMA essentially performs this function, but does so repeatedly, *d* times, in order to reduce a non-stationary series to a stationary one.  
In order to handle other forms of non-stationarity beyond stochastic trends additional models can be used.  
Seasonality effects (such as those that occur in commodity prices) can be tackled with the Seasonal ARIMA model (SARIMA), however we won't be discussing SARIMA much in this series.  
Conditional heteroscedastic effects (as with volatility clustering in equities indexes) can be tackled with ARCH/GARCH.  

Prior to defining ARIMA processes we need to discuss the concept of an integrated series:  

**Integrated Series of order *d* **  
A time series {$x_{t}$} is integrated of order *d*, *I(d)*, if:
$$
\Delta^{d}{x_{t}}=w_{t}
$$
Alternatively, using the Backward Shift Operator $B$ an equivalent condition is:  
$$
(1-B^{d})x_{t}=w_{t}
$$
That is, if we difference the series *d*  times we receive a discrete white noise series.  

Now that we have defined an integrated series we can define the ARIMA process itself:  

**Autoregressive Integrated Moving Average Model of order p, d, q**  
A time series {$x_{t}$} is an autoregressive integrated moving average model of order p, d, q, ARIMA(p,d,q), if $\Delta^{d}{x_{t}}$ is an autoregressive moving average of order p,q, ARMA(p,q).  
That is, if the series {$x_{t}$} is differenced *d* times, and it then follows an ARMA(p,q) process, then it is an ARIMA(p,d,q) series.  
then an ARIMA(p,d,q) process can be written in terms of the Backward Shift Operator, $B$:
$$
\theta_{p}(B)(1-B)^{d}x_{t}=\phi_{q}(B)w_{t}
$$
Where,  

* $w_{t}$is a discrete white noise series  
* $\theta_{p}$ is the function of autoregressive model  
* $\phi_{q}$ is the function of moving average model  

There are some points to note about these definitions  
Since the random walk is given by $x_{t}=x_{t-1}+w_{t}$ it can be seen that *I(1)*  is another representation, since $\Delta^{1}{x_{t}}=w_{t}$.  
If we suspect a non-linear trend then we might be able to use repeated differencing (i.e. *d* >1) to reduce a series to stationary white noise.  

Now, we apply model for this first we divide our time series data into train or test time series.

## Data Partitioning
```{r}
train<-Time_series[1:1300]
test<-Time_series[1301:1330]
```
As, we discussed above we need a stationary series for this model. So, we check stationary of our time series using **Augmented Dickey-Fuller test(ADF test)**.  

## d-value
```{r}
ADF1<-adf.test(train)  ;ADF1
```
As, we see p-value > 0.05  
So, this series is not a stationary series. we change this series into a stationary series using *diff* function and further check for for its stationarity  


```{r}
train1<-diff(train)
ADF2<-adf.test(diff(train1))  ;ADF2
```
p-value < 0.05, so this series is a stationary series.
```{r}
par(mfrow=c(2,1))
plot(train1,type = "l",main="Difference 1")
plot(diff(train1),type = "l",main="Difference 2")
```
As we see that their is no much difference in both the graph so using this graph we also say that *train* series is stationary after 1st differencing.  
Now, we find the (p,q) value for our model. We can find this using *auto.arima* command but I'll use the another way:

## (p,q) value
```{r}
final.aic <- Inf
final.order <- c(0,0,0)
for (p in 1:6)  for (q in 1:6) {
  current.aic <- AIC(arima(train1, order=c(p, 1, q)))
  if (current.aic < final.aic) {
    final.aic <- current.aic
    final.order <- c(p, 1, q)
    final.arima <- arima(train1, order=final.order)
  }
}
final.order
```
So, we obtain all measure for fitting an arima model.

## ARIMA Model
```{r}
arima_model<-arima(train,order=c(5,1,6))
summary(arima_model)
```
## Analysis of Model on Train data
```{r}
"predict_train"=train-arima_model$residuals
tail(data.frame("actual"=train,predict_train,"Error"=arima_model$residuals),10)
arima_g1<-ggplot()+ggtitle("Actual Value v/s Model Value")+
  geom_line(aes(x=1:1300,y = train),col="red")+
  geom_line(aes(x=1:1300,y=predict_train),col="blue")+
  xlab("Index")+ylab("Close Price")
arima_g2<-ggplot()+ggtitle("Residuals")+
  geom_line(aes(x=1:1300,y=arima_model$residuals),col="brown")+
  xlab("Index")+ylab("Close Price")
grid.arrange(arima_g1,arima_g2,ncol=2)
accuracy(arima_model)
```
on seeing the graphics or accuracy measures we can say that this model is quit good fitted
We can plot the residuals of the fitted model to see if we have evidence of discrete white noise:
```{r}
par(mfrow=c(1,1),mai=c(1,1,0.75,0))
acf(resid(arima_model))
pacf(resid(arima_model))
checkresiduals(arima_model_pred,test=F)
```
In this graph we see that our residuals of model is concentrated towards 0. We see from graphics that our model is good fitted. Lets conform it from Ljung-Box test.
```{r}
Box.test(resid(arima_model),lag=35,type="Ljung-Box")
```
Since the p-value is greater than 0.05 we have evidence of a good model fit.

## Analysis on Test Data  
```{r}
arima_model.test<-Arima(test,model=arima_model)
data.frame("Actual"=arima_model.test$x,"Predict"=arima_model.test$fitted,
           "Error"=arima_model.test$residuals)
date_seq30<-seq(as.Date("2021/06/10"),by = "days",length.out=30)
arima_g3<-ggplot()+ggtitle("Actual Value v/s Model Value")+
  geom_line(aes(x=date_seq30,y = arima_model.test$x),col="red")+
  geom_line(aes(x=date_seq30,y=arima_model.test$fitted),col="blue")+
  xlab("Date")+ylab("Close Price")
arima_g4<-ggplot()+ggtitle("Residual")+
  geom_line(aes(x=date_seq30,y=arima_model.test$residuals),col="brown")+
  xlab("Date")+ylab("Close Price")
grid.arrange(arima_g3,arima_g4,ncol=2)
accuracy(arima_model.test)
```
On Test  data we see that model predict a good trend for 30 days.

## Model Prediction for 30 Days
```{r}
arima_model_pred<-arima(Time_series,order=c(5,1,6))
arima_forecast<-forecast(model1,h=30)
arima_forecast$mean
autoplot(arima_forecast)
```
Clearly, we see that ARIMA model give a constant trend for last 30 days


# Conclusion










